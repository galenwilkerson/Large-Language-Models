{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d371ae22",
   "metadata": {},
   "source": [
    "# Shakespearian Dialog\n",
    "\n",
    "Let's try to use a Large Language Model to implement a project where two chatbots are speaking to each other as comic characters from Shakespeare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91a5b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d72873",
   "metadata": {},
   "source": [
    "## Use an efficient, distilled GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402fc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ba5492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: The first thing I want to say to you is that I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not\n",
      "Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am\n",
      "Falstaff: Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game.\n",
      "Bottom: Falstaff: Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of\n",
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: The first thing I want to say to you is that I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not\n",
      "Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am\n",
      "Falstaff: Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game.\n",
      "Bottom: Falstaff: Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, num_exchange=5, max_response_length=50):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters, with each response restricted in length.\n",
    "\n",
    "    Args:\n",
    "        character1 (str): Name of the first character.\n",
    "        character2 (str): Name of the second character.\n",
    "        start_sentence (str): The initial sentence to start the conversation.\n",
    "        model: Pre-trained GPT-2 model.\n",
    "        tokenizer: GPT-2 tokenizer.\n",
    "        num_exchange (int): Number of exchanges between the characters.\n",
    "        max_response_length (int): Maximum length of each response.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    conversation = f\"{character1}: {start_sentence}\\n\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        input_ids = tokenizer.encode(conversation, return_tensors='pt')\n",
    "        # Set the maximum length for the model generation\n",
    "        max_length_for_generation = len(input_ids[0]) + max_response_length\n",
    "        output = model.generate(input_ids, max_length=max_length_for_generation, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        last_newline = response.rfind('\\n')\n",
    "        last_response = response[last_newline:].strip()  # Extract only the last response\n",
    "\n",
    "        conversation_response = f\"{current_character}: {last_response}\"\n",
    "        print(conversation_response)\n",
    "        conversation += conversation_response + \"\\n\"\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", \n",
    "                                  model, tokenizer)\n",
    "print(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a29e68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galen/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom: \n",
      "Falstaff: The first thing I want to say to you is that I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not\n",
      "Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am\n",
      "Falstaff: Bottom: Falstaff: I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conversation\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Start the conversation\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mshakespearean_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFalstaff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBottom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat say you, merry sir, on this fine evening?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(conversation)\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mshakespearean_chat\u001b[0;34m(character1, character2, start_sentence, model, tokenizer, num_exchange, max_response_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(conversation, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m max_length_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m max_response_length\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length_for_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m last_newline \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1458\u001b[0m         input_ids,\n\u001b[1;32m   1459\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2335\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2332\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2335\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2343\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1096\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1094\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1096\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, num_exchange=5, max_response_length=50):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters, with improved generation to avoid repetition.\n",
    "\n",
    "    Args:\n",
    "        character1, character2, start_sentence, model, tokenizer: Same as before.\n",
    "        num_exchange (int): Number of exchanges between the characters.\n",
    "        max_response_length (int): Maximum length of each response.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    conversation = f\"{character1}: {start_sentence}\\n\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        input_ids = tokenizer.encode(conversation, return_tensors='pt')\n",
    "        max_length_for_generation = len(input_ids[0]) + max_response_length\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length_for_generation, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,  # Adjust as needed\n",
    "            top_k=50  # Adjust as needed\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        last_newline = response.rfind('\\n')\n",
    "        last_response = response[last_newline:].strip()\n",
    "\n",
    "        # Trim the conversation to the last few lines if it becomes too long\n",
    "        if len(conversation.split('\\n')) > 6:\n",
    "            conversation = '\\n'.join(conversation.split('\\n')[-3:])\n",
    "\n",
    "        conversation_response = f\"{current_character}: {last_response}\"\n",
    "        print(conversation_response)\n",
    "        conversation += conversation_response + \"\\n\"\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", \n",
    "                                  model, tokenizer)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a407169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening? <|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galen/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Falstaff: What say you, merry sir, on this fine evening? <|endoftext|>\n",
      "Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?” <|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, num_exchange=5, max_response_length=50):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters with strategies to reduce repetition.\n",
    "\n",
    "    Args:\n",
    "        character1, character2, start_sentence, model, tokenizer: Same as before.\n",
    "        num_exchange (int): Number of exchanges.\n",
    "        max_response_length (int): Maximum length of each response.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    stop_token = \"<|endoftext|>\"\n",
    "    conversation = f\"{character1}: {start_sentence} {stop_token}\\n\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        # Use only the last part of the conversation as context\n",
    "        input_context = conversation.split(stop_token)[-2].strip()\n",
    "        input_ids = tokenizer.encode(input_context, return_tensors='pt')\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + max_response_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.encode(stop_token)[0]\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = response.split(stop_token)[0].strip()  # Stop at the stop token\n",
    "\n",
    "        conversation_response = f\"{current_character}: {response} {stop_token}\"\n",
    "        print(conversation_response)\n",
    "        conversation += conversation_response + \"\\n\"\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", \n",
    "                                  model, tokenizer)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23811f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening? <|endoftext|>\n",
      "Bottom: Falstaff: What say you, merry sir, on this fine evening? <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other family or friends of my family.\n",
      "I can't wait to see what the next day will be for my family.\n",
      "I hope that the next day's time will be the same as my last.\n",
      "Advertisements <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other family or friends of my family.\n",
      "I can't wait to see what the next day will be for my family.\n",
      "I hope that the next day's time will be the same as my last.\n",
      "Advertisements <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other family or friends of my family.\n",
      "I can't wait to see what the next day will be for my family.\n",
      "I hope that the next day's time will be the same as my last.\n",
      "Advertisements <|endoftext|>\n",
      "Falstaff: What say you, merry sir, on this fine evening? <|endoftext|>\n",
      "Bottom: Falstaff: What say you, merry sir, on this fine evening? <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other family or friends of my family.\n",
      "I can't wait to see what the next day will be for my family.\n",
      "I hope that the next day's time will be the same as my last.\n",
      "Advertisements <|endoftext|>\n",
      "Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other family or friends of my family.\n",
      "I can't wait to see what the next day will be for my family.\n",
      "I hope that the next day's time will be the same as my last.\n",
      "Advertisements <|endoftext|>\n",
      "Bottom: Falstaff: Bottom: Falstaff: Bottom: Falstaff: What say you, merry sir, on this fine evening?\n",
      "\n",
      "\n",
      "I know that it will be nice to see the other people of my family and friends, and I am sure that I will enjoy the very best of my time.\n",
      "I do not think that it will be an issue for any other family or friends of my family.\n",
      "I can't wait to see what the next day will be for my family.\n",
      "I hope that the next day's time will be the same as my last.\n",
      "Advertisements <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, num_exchange=5, max_response_length=50):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters with improved generation settings.\n",
    "\n",
    "    Args:\n",
    "        character1, character2, start_sentence, model, tokenizer: Same as before.\n",
    "        num_exchange (int): Number of exchanges.\n",
    "        max_response_length (int): Maximum length of each response.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    stop_token = \"<|endoftext|>\"\n",
    "    conversation = f\"{character1}: {start_sentence} {stop_token}\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        # Use only the last part of the conversation as context\n",
    "        input_context = conversation.split(stop_token)[-2].strip()\n",
    "        input_ids = tokenizer.encode(input_context, return_tensors='pt')\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + max_response_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.encode(stop_token)[0],\n",
    "            do_sample=True  # Enable sampling\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = response.split(stop_token)[0].strip()  # Stop at the stop token\n",
    "\n",
    "        conversation_response = f\"{current_character}: {response} {stop_token}\"\n",
    "        print(conversation_response)\n",
    "        conversation += \"\\n\" + conversation_response\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", \n",
    "                                  model, tokenizer)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22adc7f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: Advertisements\n",
      "Bottom: My favorite part about this book is how much it's been. The story goes from the first chapter to the final chapter. There's so much to see in the story. It's so much more than you could imagine in a book. You can see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds. It's so much more than you could imagine in a book. You can see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are\n",
      "Falstaff: There is so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters\n",
      "Bottom: Falstaff: So you're probably right. There are so many characters out there that you can tell the story\n",
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: Advertisements\n",
      "Bottom: My favorite part about this book is how much it's been. The story goes from the first chapter to the final chapter. There's so much to see in the story. It's so much more than you could imagine in a book. You can see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds. It's so much more than you could imagine in a book. You can see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are\n",
      "Falstaff: There is so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters are in the same person with different backgrounds, and there's so much more to see in the book that all the characters\n",
      "Bottom: Falstaff: So you're probably right. There are so many characters out there that you can tell the story\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, num_exchange=5):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters with noise to prevent repetition.\n",
    "\n",
    "    Args:\n",
    "        character1 (str): Name of the first character.\n",
    "        character2 (str): Name of the second character.\n",
    "        start_sentence (str): The initial sentence to start the conversation.\n",
    "        model: Pre-trained GPT-2 model.\n",
    "        tokenizer: GPT-2 tokenizer.\n",
    "        num_exchange (int): Number of exchanges between the characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    conversation = f\"{character1}: {start_sentence}\\n\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        # Inject some randomness in temperature and top_p\n",
    "        temperature = random.uniform(0.7, 1.0)\n",
    "        top_p = random.uniform(0.8, 0.95)\n",
    "\n",
    "        input_ids = tokenizer.encode(conversation, return_tensors='pt')\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=1000, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        last_newline = response.rfind('\\n')\n",
    "        last_response = response[last_newline:].strip()\n",
    "\n",
    "        conversation_response = f\"{current_character}: {last_response}\"\n",
    "        print(conversation_response)\n",
    "        conversation += conversation_response + \"\\n\"\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", \n",
    "                                  model, tokenizer)\n",
    "print(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3151346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: Battles for the\n",
      "Bottom: Sell: I know you\n",
      "Falstaff: Bottom\n",
      "Bottom: Falstaff: I didn't get to\n",
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: Battles for the\n",
      "Bottom: Sell: I know you\n",
      "Falstaff: Bottom\n",
      "Bottom: Falstaff: I didn't get to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, \n",
    "                       num_exchange=5, max_response_length=50):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters with controlled response length.\n",
    "\n",
    "    Args:\n",
    "        character1 (str): Name of the first character.\n",
    "        character2 (str): Name of the second character.\n",
    "        start_sentence (str): The initial sentence to start the conversation.\n",
    "        model: Pre-trained GPT-2 model.\n",
    "        tokenizer: GPT-2 tokenizer.\n",
    "        num_exchange (int): Number of exchanges between the characters.\n",
    "        max_response_length (int): Maximum length of each response.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    conversation = f\"{character1}: {start_sentence}\\n\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        # Inject some randomness in temperature and top_p\n",
    "        temperature = random.uniform(0.7, 1.0)\n",
    "        top_p = random.uniform(0.8, 0.95)\n",
    "\n",
    "        input_ids = tokenizer.encode(conversation, return_tensors='pt')\n",
    "        # Calculate max length for the model generation\n",
    "        max_length_for_generation = len(input_ids[0]) + max_response_length\n",
    "\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length_for_generation, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        last_newline = response.rfind('\\n')\n",
    "        last_response = response[last_newline:].strip()\n",
    "\n",
    "        conversation_response = f\"{current_character}: {last_response}\"\n",
    "        print(conversation_response)\n",
    "        conversation += conversation_response + \"\\n\"\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", model, tokenizer)\n",
    "print(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa6252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: Ludwig said, \"I have no idea how many times you have received this letter. It's been a year and a half. I've heard that many times, Mr.\n",
      "Bottom: Miner: But it's a long time.\n",
      "Falstaff: Bottom: Mr.\n",
      "Bottom: Falstaff: That\n",
      "Falstaff: What say you, merry sir, on this fine evening?\n",
      "Bottom: \n",
      "Falstaff: Ludwig said, \"I have no idea how many times you have received this letter. It's been a year and a half. I've heard that many times, Mr.\n",
      "Bottom: Miner: But it's a long time.\n",
      "Falstaff: Bottom: Mr.\n",
      "Bottom: Falstaff: That\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def shakespearean_chat(character1, character2, start_sentence, model, tokenizer, num_exchange=5, max_response_length=50):\n",
    "    \"\"\"\n",
    "    Simulates a conversation between two Shakespearean characters, ensuring complete sentences.\n",
    "\n",
    "    Args:\n",
    "        character1 (str): Name of the first character.\n",
    "        character2 (str): Name of the second character.\n",
    "        start_sentence (str): The initial sentence to start the conversation.\n",
    "        model: Pre-trained GPT-2 model.\n",
    "        tokenizer: GPT-2 tokenizer.\n",
    "        num_exchange (int): Number of exchanges between the characters.\n",
    "        max_response_length (int): Maximum length of each response.\n",
    "\n",
    "    Returns:\n",
    "        str: The simulated conversation.\n",
    "    \"\"\"\n",
    "    conversation = f\"{character1}: {start_sentence}\\n\"\n",
    "    print(conversation.strip())\n",
    "    current_character = character2\n",
    "\n",
    "    for i in range(num_exchange):\n",
    "        temperature = random.uniform(0.7, 1.0)\n",
    "        top_p = random.uniform(0.8, 0.95)\n",
    "\n",
    "        input_ids = tokenizer.encode(conversation, return_tensors='pt')\n",
    "        max_length_for_generation = min(len(input_ids[0]) + max_response_length, 1024)\n",
    "\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length_for_generation, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        last_newline = response.rfind('\\n')\n",
    "        last_response = response[last_newline:].strip()\n",
    "\n",
    "        # Trim to the last complete sentence within a reasonable length\n",
    "        match = re.search(r'([.!?])[^.!?]*$', last_response)\n",
    "        if match:\n",
    "            end_index = match.start()\n",
    "            last_response = last_response[:end_index+1]\n",
    "\n",
    "        conversation_response = f\"{current_character}: {last_response}\"\n",
    "        print(conversation_response)\n",
    "        conversation += conversation_response + \"\\n\"\n",
    "        current_character = character1 if current_character == character2 else character2\n",
    "\n",
    "    return conversation\n",
    "\n",
    "# Start the conversation\n",
    "conversation = shakespearean_chat(\"Falstaff\", \"Bottom\", \"What say you, merry sir, on this fine evening?\", model, tokenizer)\n",
    "print(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618fa96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
